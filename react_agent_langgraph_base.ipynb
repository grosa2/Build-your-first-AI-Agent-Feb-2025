{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a basic LLM Agent using Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# From https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from typing import Literal\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Params\n",
    "#\n",
    "OPENAI_MODEL_NAME = \"gpt-4o-mini\"\n",
    "OPENAI_MODEL_TEMPERATURE = 0\n",
    "\n",
    "#\n",
    "# Initialize OpenAI\n",
    "#\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL_NAME, temperature=OPENAI_MODEL_TEMPERATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define the tools that will be available to the agent. The description of each tool will be used by the agent to understand what the tool does.\n",
    "#\n",
    "\n",
    "@tool\n",
    "def calculate(formula):\n",
    "    \"\"\"Use this to run math calculations. For example, calculate: 4 * 7 / 3\"\"\"\n",
    "    return eval(formula)\n",
    "\n",
    "tools = [calculate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "# Focus\n",
    "\n",
    "# Steps\n",
    "\n",
    "\n",
    "# Notes\n",
    "- Answer using the same language as the customer.\n",
    "- Avoid making assumptions, ask questions to clarify any doubts.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages.system import SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "#\n",
    "# The built-in create_react_agent function embeds all the logic needed to create a basic ReAct agent\n",
    "#\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = create_react_agent(model=llm, tools=tools, state_modifier=prompt, checkpointer=memory, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define an utility function to print the stream of messages\n",
    "#\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "\n",
    "def ask_assistant(question: str, config: dict = None):\n",
    "    inputs = {\"messages\": [(\"user\", question)]}\n",
    "    print_stream(graph.stream(inputs, config=config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Desidero spedire una grande scultura delicata che misura circa 244 cm di altezza', additional_kwargs={}, response_metadata={}, id='0fe340a5-ced1-47c1-bc66-c15fd8bab82e'),\n",
       "  AIMessage(content='Thought: La scultura è alta 244 cm, quindi dovrò considerare container che possano ospitare oggetti alti. Dovrei chiedere ulteriori dettagli sulla larghezza, profondità e peso della scultura, oltre a sapere se ci sono requisiti specifici per la protezione o il trasporto. \\n\\nChiederò quindi informazioni aggiuntive sulla larghezza, profondità e peso della scultura.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 1991, 'total_tokens': 2081, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d6b477bb-cadc-493e-9805-0d5d5c367d80-0', usage_metadata={'input_tokens': 1991, 'output_tokens': 90, 'total_tokens': 2081, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "question = \"Desidero spedire una grande scultura delicata che misura circa 244 cm di altezza\"\n",
    "inputs = {\"messages\": [(\"user\", question)]}\n",
    "graph.invoke(inputs, config=config, stream_mode=\"values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execute the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set conversation thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Desidero spedire una grande scultura delicata che misura circa 244 cm di altezza e necessita di spazio extra oltre a una manipolazione sicura, mantenendo una temperatura controllata; quale container sarebbe il più adatto per questa spedizione?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thought: La scultura è alta 244 cm e richiede spazio extra e una manipolazione sicura, oltre a un controllo della temperatura. Dato che la scultura è delicata, un container refrigerato potrebbe essere la scelta migliore. Tuttavia, devo anche considerare le dimensioni e il peso della scultura per assicurarmi che possa adattarsi al container. \n",
      "\n",
      "Chiederò quindi informazioni sul peso della scultura e sulle sue dimensioni in larghezza e profondità per determinare il container più adatto.\n"
     ]
    }
   ],
   "source": [
    "ask_assistant(question, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cli for chat using a while loop\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question.startswith(\"/bye\") or not question.strip():\n",
    "        break\n",
    "    ask_assistant(question, config=config)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oci-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
